{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d243541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO, BytesIO\n",
    "from datetime import datetime, timedelta\n",
    "import uuid\n",
    "import os\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20c20ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapter Layer\n",
    "#function converting CSV into DataFrame\n",
    "def read_csv_to_dataframe(key_id, bucket_obj, decod='utf-8', dlm=','):\n",
    "    csv_obj=bucket_obj.Object(key=key_id).get()['Body'].read().decode(decod)\n",
    "    data=StringIO(csv_obj)\n",
    "    return pd.read_csv(data,delimiter=dlm)\n",
    "\n",
    "#writing to S3\n",
    "#Convertign dataframe into parquet byte file and saving it on S3\n",
    "def write_to_s3(bucket_obj,df_obj, key_id, file_type='parquet'):\n",
    "    if file_type=='parquet':\n",
    "        out_buffer=BytesIO()\n",
    "        df_obj.to_parquet(out_buffer, index=False)\n",
    "        bucket_obj.put_object(Body=out_buffer.getvalue(),Key=key_id)\n",
    "    elif file_type=='csv':\n",
    "        out_buffer=StringIO()\n",
    "        df_obj.to_csv(out_buffer, index=False)\n",
    "        bucket_obj.put_object(Body=out_buffer.getvalue(),Key=key_id)\n",
    "    return True\n",
    "\n",
    "def list_file_in_prefix(bucket_obj, prefix):\n",
    "    return [obj.key for obj in bucket_obj.objects.filter(Prefix=prefix)] \n",
    "\n",
    "def return_date_list(bucket_obj,date_format, date_start_obj, meta_key):\n",
    "    min_date=datetime.strptime(date_start_obj,date_format).date()-timedelta(days=1)\n",
    "    today=datetime.today().date()\n",
    "    try:\n",
    "        df_meta=read_csv_to_dataframe(meta_key, bucket_obj)\n",
    "        dates=[(min_date + timedelta(days=x)) for x in range(0,(today-min_date).days+1)]\n",
    "        src_date=set(pd.to_datetime(df_meta['sourced_date']).dt.date)\n",
    "        dist_list=set(dates[1:])-src_date\n",
    "        if dist_list:\n",
    "            min_date=min(set(dates[1:])-src_date)- timedelta(days=1)\n",
    "            return_dates = [date.strftime(src_format) for date in dates if date>=min_date]\n",
    "            return_min_date=(min_date+timedelta(days=1)).strftime(src_format)\n",
    "        else:\n",
    "            return_dates=[]\n",
    "            return_min_date=datetime(2200,1,1).date()\n",
    "    except :\n",
    "        return_dates=[(min_date+timedelta(days=x)).strftime(date_format) for x in range(0,(today-min_date).days+1)]\n",
    "        return_min_date=date_start_obj\n",
    "    return return_min_date, return_dates\n",
    "            \n",
    "#returning list of all objects from AWS S3 bucket\n",
    "def list_of_S3_obj(bucket_obj,date_format,date_start_obj):\n",
    "    min_date=datetime.strptime(date_start_obj,date_format).date()-timedelta(days=1)\n",
    "    return [obj for obj in bucket_obj.objects.all() if datetime.strptime(obj.key.split('/')[0],date_format).date()>=min_date]\n",
    "\n",
    "#reading from parquet byte fules and creating pandas DataFrame\n",
    "def from_prq_to_dataframe(key_id,bucket_id):\n",
    "    prq_file=bucket_id.Object(key=key_id).get()['Body'].read()\n",
    "    data = BytesIO(prq_file)\n",
    "    return pd.read_parquet(data)\n",
    "\n",
    "#saving to Azure Blob Storage\n",
    "def write_to_blob(blob_service_client, container_id, blob_id, df_obj):\n",
    "    out_buffer=BytesIO()\n",
    "    df_obj.to_parquet(out_buffer, index=False)\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_id, blob=blob_id)\n",
    "    blob_client.upload_blob(out_buffer.getvalue(),overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7fbae6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Application Layer\n",
    "def extract(bucket_obj, date_list):\n",
    "    files = [key for date in date_list for key in list_file_in_prefix(bucket_obj,date)]\n",
    "    return pd.concat([read_csv_to_dataframe(obj, bucket_obj) for obj in files], ignore_index=True)\n",
    "\n",
    "def transform_report1(df_obj,start_date,column_list):\n",
    "    #part of code to select only required columns\n",
    "    df_obj=df_obj.loc[:,column_list]\n",
    "    df_obj.dropna(inplace=True)\n",
    "    df_obj['opening_price']=df_obj.sort_values(by=['Time']).groupby(['ISIN','Date'])['StartPrice'].transform('first')\n",
    "    df_obj['closing_price']=df_obj.sort_values(by=['Time']).groupby(['ISIN','Date'])['EndPrice'].transform('last')\n",
    "    df_obj=df_obj.groupby(['ISIN','Date'], as_index=False).agg(opening_price_eur=('opening_price','min'), closing_price_eur=('closing_price','max'),\n",
    "                                                           minimum_price_eur=('MinPrice','min'), maximum_price_eur=('MaxPrice','max'), daily_traded_volume=('TradedVolume','sum'))\n",
    "    #percent change prev closing\n",
    "    df_obj['prev_closing_price']=df_obj.sort_values(by=['Date']).groupby(['ISIN'])['closing_price_eur'].shift(1)\n",
    "    df_obj['change_vs_prior_day']=(df_obj['closing_price_eur']-df_obj['prev_closing_price'])/df_obj['prev_closing_price']*100\n",
    "    df_obj.drop(columns=['prev_closing_price'],inplace=True)\n",
    "    df_obj=df_obj.round(decimals=2)\n",
    "    \n",
    "    #filtering\n",
    "    df_obj=df_obj[df_obj.Date>=start_date]\n",
    "    return df_obj\n",
    "\n",
    "def update_metafile(bucket_obj, meta_key, extract_date_list):\n",
    "    df_new=pd.DataFrame(columns=['sourced_date','date_of_processing'])\n",
    "    df_new['sourced_date']=extract_date_list\n",
    "    df_new['date_of_processing']=datetime.today().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    df_meta=read_csv_to_dataframe(meta_key, bucket_obj)\n",
    "    df_all=pd.concat([df_new,df_meta], ignore_index=True)\n",
    "    write_to_s3(bucket_obj,df_all,meta_key,'csv')\n",
    "    return True;\n",
    "\n",
    "def load(bucket_obj,df_obj, file_name, file_format):\n",
    "    key = file_name + datetime.today().strftime(\"%Y%m%d_%H%M%S\") + file_format\n",
    "    write_to_s3(bucket_obj,df_obj,key)\n",
    "    return True\n",
    "\n",
    "def etl_report1(bucket,bucket_target, date_list, column_list,start_date, file_name,file_format):\n",
    "    df=extract(bucket, date_list)\n",
    "    df=transform_report1(df,start_date,column_list)\n",
    "    load(bucket_target,df,file_name,file_format)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7f2923c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function entrypoint\n",
    "def main():\n",
    "    #Parameters/Configurations\n",
    "    #Later from Config\n",
    "    arg_date='2022-09-01'\n",
    "    meta_key='meta_file.csv'\n",
    "    src_format='%Y-%m-%d'\n",
    "    source_bucket_name='xetra-1234'\n",
    "    target_bucket_name='radek-xetra-1234'\n",
    "    init_key_name='2022-01-07/2022-01-07_BINS_XETR22.csv'\n",
    "    trg_key='xetra_daily_report_'\n",
    "    trg_format='.parquet'\n",
    "    #hardcoding list of columns\n",
    "    Final_columns=['ISIN', 'Date', 'Time', 'StartPrice', 'MaxPrice', 'MinPrice', 'EndPrice', 'TradedVolume']\n",
    "    \n",
    "    #Init\n",
    "    s3=boto3.resource('s3')\n",
    "    bucket=s3.Bucket(source_bucket_name)\n",
    "    bucket_target=s3.Bucket(target_bucket_name)\n",
    "    \n",
    "    #run job\n",
    "    extract_date, date_list=return_date_list(bucket_target,src_format,arg_date,meta_key)\n",
    "    etl_report1(bucket, bucket_target, date_list, Final_columns,extract_date, trg_key, trg_format)\n",
    "    update_metafile(bucket_target, meta_key, list(set(date_list)-set(extract_date)))\n",
    "                    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1dcdf170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connection to Azure Storage Account\n",
    "connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "container_name='radek-xetra-1234'\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "container_client=blob_service_client.get_container_client(container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f1ce79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report=pd.concat([from_prq_to_dataframe(obj.key, bucket_target) for obj in bucket_target.objects.all()], ignore_index=True)\n",
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f7480375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sourced_date</th>\n",
       "      <th>date_of_processing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-08-31</td>\n",
       "      <td>20220913_044426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>20220913_044426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-02</td>\n",
       "      <td>20220913_044426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-03</td>\n",
       "      <td>20220913_044426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-09-04</td>\n",
       "      <td>20220913_044426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-09-05</td>\n",
       "      <td>20220913_044426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2022-09-06</td>\n",
       "      <td>2022-09-06 12:33:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-06</td>\n",
       "      <td>20220913_044426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-09-07</td>\n",
       "      <td>20220913_044426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-09-07</td>\n",
       "      <td>20220913_044116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022-09-07</td>\n",
       "      <td>2022-09-07 12:33:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-09-08</td>\n",
       "      <td>20220913_044426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-09-08</td>\n",
       "      <td>20220913_044116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-09</td>\n",
       "      <td>20220913_044426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022-09-09</td>\n",
       "      <td>20220913_044116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-09-10</td>\n",
       "      <td>20220913_044426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-09-10</td>\n",
       "      <td>20220913_044116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-09-11</td>\n",
       "      <td>20220913_044426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022-09-11</td>\n",
       "      <td>20220913_044116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-09-12</td>\n",
       "      <td>20220913_044116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-09-12</td>\n",
       "      <td>20220913_044426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-09-13</td>\n",
       "      <td>20220913_044426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2022-09-13</td>\n",
       "      <td>20220913_044116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sourced_date    date_of_processing\n",
       "11   2022-08-31       20220913_044426\n",
       "1    2022-09-01       20220913_044426\n",
       "4    2022-09-02       20220913_044426\n",
       "2    2022-09-03       20220913_044426\n",
       "8    2022-09-04       20220913_044426\n",
       "12   2022-09-05       20220913_044426\n",
       "21   2022-09-06   2022-09-06 12:33:23\n",
       "0    2022-09-06       20220913_044426\n",
       "6    2022-09-07       20220913_044426\n",
       "16   2022-09-07       20220913_044116\n",
       "22   2022-09-07   2022-09-07 12:33:23\n",
       "9    2022-09-08       20220913_044426\n",
       "18   2022-09-08       20220913_044116\n",
       "3    2022-09-09       20220913_044426\n",
       "14   2022-09-09       20220913_044116\n",
       "5    2022-09-10       20220913_044426\n",
       "15   2022-09-10       20220913_044116\n",
       "10   2022-09-11       20220913_044426\n",
       "19   2022-09-11       20220913_044116\n",
       "17   2022-09-12       20220913_044116\n",
       "7    2022-09-12       20220913_044426\n",
       "13   2022-09-13       20220913_044426\n",
       "20   2022-09-13       20220913_044116"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_date='2022-09-06'\n",
    "s3=boto3.resource('s3')\n",
    "meta_key='meta_file.csv'\n",
    "target_bucket_name='radek-xetra-1234'\n",
    "bucket_target=s3.Bucket(target_bucket_name)\n",
    "src_format='%Y-%m-%d'\n",
    "\n",
    "#df_report=from_prq_to_dataframe('xetra_daily_report_20220913_040833.parquet', bucket_target)\n",
    "df_meta=read_csv_to_dataframe(meta_key, bucket_target)\n",
    "df_meta.sort_values(['sourced_date'])\n",
    "#return_date_list(bucket_target,src_format,arg_date,meta_key)\n",
    "#df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9e7cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_blob(blob_service_client, container_name, key, df_new_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e6e9923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing all blobs in container\n",
    "blob_list = container_client.list_blobs()\n",
    "for blob in blob_list:\n",
    "    print(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d56983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening a blob client and downloading a specific blob\n",
    "blob_client= blob_service_client.get_container_client(container= container_name) \n",
    "prq_file=blob_client.download_blob(blob_name).readall()\n",
    "data = BytesIO(prq_file)\n",
    "df1 = pd.read_parquet(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02105fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1=['aa','bb','cc']\n",
    "str='aa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f94188a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1.remove(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7327f11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bb', 'cc']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963d7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
